# Hugging Face access token for model access
HUGGINGFACE_TOKEN=

# Conda environment name, please change if you would like to use a different name
CONDA_ENV_NAME=ovlangvidsumm

# OVMS endpoint for all models
OVMS_CONDA_ENV_NAME=ovms_env
OVMS_ENDPOINT="http://localhost:8013/v3/chat/completions"

####### Video ingestion configuration
OBJ_DETECT_ENABLED="TRUE"
OBJ_DETECT_MODEL_PATH="ov_dfine/dfine-s-coco.xml"
OBJ_DETECT_SAMPLE_RATE=5
OBJ_DETECT_THRESHOLD=0.9

####### Summary merger configuration

# Name of the LLM model for summary merging in Hugging Face format (model runs on OVMS model server)
LLAMA_MODEL="meta-llama/Llama-3.2-3B-Instruct"

# Device for the summary merger model: CPU, GPU or NPU
SUMMARY_MERGER_LLM_DEVICE="GPU"

# Prompt for merging multiple chunk summaries into one summary
SUMMARY_PROMPT='You are a video summarization agent. Your job is to merge multiple chunk summaries into one balanced and complete summary.
Important: Treat all summaries as equally important. Do not prioritize summaries that have more detail or mention suspicious activity - your goal is to combine information, not amplify it.

Guidelines:
- Extract the most important insights from all summaries into a concise output.
- Give equal importance to each chunk, regardless of its length or uniqueness.
- Estimate the number of people in the scene based on textual hints. If summaries mention no people or say "empty" or "no visible customers", count that as 0 people. If someone is mentioned (e.g., "a customer", "a person walks in"), count them.
- Only output one summary in the exact format below.
- Do not include the input summaries or instructions in your response.

Output Format:
Overall Summary: <summary here>
Activity Observed: <bullet points>
Potential Suspicious Activity: <suspicious behavior if any>
Number of people in the scene: <best estimate. DO NOT overcount>.
Anomaly Score: <float from 0 to 1, based on severity of suspicious activity>.
'

####### Embedding model configuration
# Currently verified model
EMBEDDING_MODEL="Salesforce/blip-itm-base-coco"

# Device for text embeddings: CPU, GPU
# Important Note: If you are changing the device here, please make sure to delete the old blip model files (bin and xml files in current dir)
TXT_EMBEDDING_DEVICE="GPU"

# Device for img embeddings: CPU, GPU, NPU 
# Important Note: If you are changing the device here, please make sure to delete the old blip model files (bin and xml files in current dir)
IMG_EMBEDDING_DEVICE="GPU"

####### Video summarization configuration
# Input video file, resolution, and prompt for summarization

# VLM model
VLM_MODEL="openbmb/MiniCPM-V-2_6"

# Device for the VLM model: CPU, GPU
VLM_DEVICE="GPU"

INPUT_FILE="one-by-one-person-detection.mp4"

RESOLUTION_X=480
RESOLUTION_Y=270

PROMPT='As an expert investigator, please analyze this video. Summarize the video, highlighting any shoplifting or suspicious activity. The output must contain the following 3 sections: Overall Summary, Activity Observed, Potential Suspicious Activity. It should be formatted similar to the following example:

**Overall Summary**
Here is a detailed description of the video.

**Activity Observed**
1) Here is a bullet point list of the activities observed. If nothing is observed, say so, and the list should have no more than 10 items.

**Potential Suspicious Activity**
1) Here is a bullet point list of suspicious behavior (if any) to highlight.
'

####### Parameters for Milvus

MILVUS_HOST="localhost"
MILVUS_PORT=19530
MILVUS_DBNAME="milvus_db"
COLLECTION_NAME="video_chunks"

####### Parameters for summarization with --run_rag option

# Query text to search for in the Vector DB
# Example: "woman shoplifting"
QUERY_TEXT=
# Query image to search for in the Vector DB (path to the image file)
QUERY_IMG=

# Optional Filter expression for the Vector DB query
# Filters can be used to narrow down the search results based on specific criteria.
# The filter expression can include:
# - mode: 'text' or 'image' (whether to search using the text summary embeddings or frame embeddings)
# - video_path: path to the video file. Search will be limited to chunks from this video.
# - chunk_path: path to the chunk file. Search will be limited to specific chunk file.
# - detected_objects: list of objects detected in the chunk, e.g., 'person', 'car', 'bag', etc. An example is provided below. 

# Examples of various types of `FILTER_EXPR` is provided below. Various supported operators can be referred to in Milvus' documentation - https://milvus.io/docs/boolean.md 
# Example: To search only text summaries: "mode=='text'". To search only frames: "mode=='image'"
# Example: To search for specific detected objects: "detected_objects LIKE '%<object name>%'"
# Example: To search on a specific video: "video_path=='<path to video>'"
# Example: Combine multiple filters using operator "and": "mode=='image' and video_path==<path to video> and detected_objects LIKE '%person%'"

### Example: FILTER_EXPR="mode=='image' and detected_objects LIKE '%person%'". Search includes only image embeddings with detected objects that contain 'person'.
FILTER_EXPR=

# Save a video clip of Milvus search result
SAVE_VIDEO_CLIP=True
# Duration of the video clip in seconds
VIDEO_CLIP_DURATION=5
# Number of top results to retrieve from Milvus
RETRIEVE_TOP_K=2